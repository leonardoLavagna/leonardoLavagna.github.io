---
title: 'Quantum Generative Modeling via Straightforward State Preparation'
date: 2024-12-12
permalink: /posts/2024/12/QGEN-SP1/
tags:
  - conference_proceeding
---

I have presented the preliminary results about the ongoing work we [@NESYA](https://sites.google.com/view/nesya) are carrying out on the relationship between state preparation techniques, sampling from the corresponding state preparation quantum circuits, and generative modeling, at the [2024 Conference on Quantum Techinques in Machine Learning](https://qtml2024.org/). Once the work is completed I will put out a comprehensive blog post, for now, I report the key points discussed at the conference.

Generative modeling has emerged as a versatile technique for producing new data samples that closely resemble a target probability distribution. It leverages large amounts of unlabeled data to train models that generate realistic data samples in fields like computer vision and speech synthesis. This blog post highlights a novel quantum approach to generative modeling, offering a simpler and potentially more efficient alternative to traditional methods.

Generative models often intersect with concepts from statistical and quantum physics. For instance, Quantum Born Machines (QBMs) utilize quantum principles to estimate probabilities across vast spaces. The core idea is to translate the generative process into learning a quantum state, leveraging quantum circuits to approximate the target probability distribution. 

The proposed approach introduces a novel quantum generative process that simplifies state preparation, minimizes trainability issues, and integrates classical optimization techniques effectively. Unlike QBMs, which require complex parameter optimization, this method employs a more straightforward framework to achieve similar or better results.

he core innovation lies in preparing a superposition quantum state \(|\psi_X\rangle\), where each basis state is weighted to approximate the target probability distribution. The process involves:

1. *Binnization*: The training dataset is iteratively subdivided, aggregating probabilities to determine the weights for each basis state.
2. *Quantum Circuit Construction*: The superposition state is created using a combination of CNOT gates and Ry rotation gates. This avoids the complexity of matrix product states (MPS) while maintaining high expressibility.

The proposed method also introduces a variant where certain angles in the quantum circuit are parameterized, allowing limited but effective optimization.

The approach was tested on benchmark datasets, including:
- *Univariate Gaussian Dataset*: The proposed model accurately recovered the distribution, outperforming QBMs in efficiency and quality of generated samples.
- *8Ã—8 MNIST Dataset*: The model successfully generated samples that closely resembled the target distribution, demonstrating versatility and scalability.

Key findings:
- The non-parameterized model performed exceptionally well without training.
- A parameterized version, when trained, achieved even greater accuracy with minimal computational overhead.

The advantages over existing models involve

- *Simplicity*: The method avoids the complexities of traditional quantum models, such as tensor network structures in MPS-based circuits.
- *Efficiency*: The model requires fewer trainable parameters and avoids gradient-based optimization pitfalls.
- **Scalability**: It generalizes well across different datasets, making it suitable for various applications.


This quantum generative modeling approach offers a powerful alternative to traditional methods, balancing simplicity, efficiency, and expressibility. By reducing the computational burden and addressing generalization issues, it paves the way for practical quantum generative models in real-world applications.

**Refs**
1. Bond-Taylor, S., et al. (2022). Deep generative modeling review. *IEEE Trans. on Pattern Analysis and Machine Intelligence*.
2. Cheng, S., et al. (2018). Information perspective on probabilistic modeling. *Entropy*.
3. Liu, J.-G., & Wang, L. (2018). Differentiable learning of quantum circuit born machines. *Physical Review A*.
4. Han, Z.-Y., et al. (2018). Unsupervised generative modeling using matrix product states. *Physical Review X*.
5. Grover, L., & Rudolph, T. (2002). Creating superpositions for probability distributions.


